Introduction
============

.. currentmodule:: hypergol

The structure of Hypergol is organised around two functions: Core functionalities that enable running Hypergol projects and code generating scripts that generate those projects.

Hypergol Data Model
-------------------

Hypergol project's domain data model is described by a set of classes that all derive from :class:`BaseData`. This provides the basic functionalities that enable the class to interoperate with the rest of the framework. The classes can be easily generated by :func:`create_data_model` then further modified. Automated tests are generated as well that check if the class still serialises properly after modification. Data model classes recursively serialise into json and stored in gzipped text files which are organised into datasets.

Datasets
--------

The :class:`Dataset` object is the primary storage format in Hypergol. It enable parallel processing by the pipeline and also interactive access from notebooks. The circumstances of the creation of the file is saved into the `.def` file: the location of the file, the type of data stored in it, the git commit and commiter. The gzipped JSON files content (before compression) is hashed with SHA1 which is then saved into ``.chk`` file so it can be verified that the file wasn't changed. The dataset's own checksum is calculated by hashing the entire ``.chk`` file with SHA1, as this contains the hash of all the files in the dataset it uniquely verifies the dataset. No part of it can be changed and have the same hash. If another dataset is used in the creation of the dataset (as in a pipeline) the hash of that dataset is added to the ``.def`` file, whose hash is added to the ``.chk`` file whose hash is added to the further downstream dataset's ``.def`` file. This way the entire processing is verified by a single hash.

Tasks
-----

Tasks are computational routines that operate on domain data. They stream objects from datasets and stream their output to disk as well, therefore make it easy to work with larger than machine memory datasets. Because datasets can be operated on by multiple threads it allow tasks to ran parallel on the same dataset. This is combined with the memory efficient data handling to achieve optimal throughput time. Completion speed is only limited by the additional memory requirements of a task.

There are three types of tasks: :class:`Source`, :class:`SimpleTask`, :class:`Task`. :class:`Source` is used to process data into the `Hypergol` framework (this is single threaded). :class:`SimpleTask` is used to process each piece of data along the pipeline (one object on the input and one object on the output). :class:`Task` is used to create new objects and datasets: (one object on the input and multiple object on the output).

Pipeline
--------

Pipelines are used to combine tasks and datasets. It manages the parallel execution and the handling of the data. It executes each task alone, so no inter-task concurency problems can happen. It also handles data versioning through git and shell execution. Logically a Pipeline is a list of Tasks that are executed sequentially and the code that handles the threads that enable execution. It is deliberately simple as opposed to other frameworks that describe computational tasks as DAGs (Airflow, Prefect.io, dask). That's because most tasks required in ML is IO/memory/CPU heavy but relatively sequential and the benefit is achieved by paralellising the slowest step in a linear pipeline. At any one time there is only one task is running and only that task's resource limits needed and can be optimised.

Testing
-------

Hypegol creates example unittests for the project so it is easy to verify assumptions about the code and extend the coverage based on the examples. Adds pylint as well, so the linting can be performed as well. The tests fail at generation time in certain cases because it is too cumbersome to autogenerate tests for some data model types. The intention here is to enable writing tests without setup and only focus on the "Given-When-Then" triple style test writing.

Modelling
---------

Hypergol provides stubs for Tensorflow models and :class:`BaseBatchProcessor` abstraction to connect the model at training and deployment to the datamodel (and datasets). To enable iterative developement and `SOLID <https://en.wikipedia.org/wiki/SOLID>`_ style developement an opinionated abstraction is provided through :class:`BaseTensorflowModel` derived from `keras.Model` and :class:`BaseTensorflowModelBlock` derived from `keras.layers.Layer`. By following the proposed structure Hypergol provides :class:`TensorflowModelManager` that handles training and evaluation, saving the model and any metrics for tensorboard. The model is packaged with the correct signature derived from the `get_outputs` function that enables deployment as well.

Deployment
----------

This is not done yet.
